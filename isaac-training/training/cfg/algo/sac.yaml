name: "sac"

feature_extractor:
  learning_rate: 5e-4
  dyn_obs_num: 5

# Stochastic Gaussian policy with Tanh-squash
actor:
  learning_rate: 3e-4
  action_limit: 2.0 # m/s

# Twin Q-value critics
critic:
  learning_rate: 1e-4
  hidden_sizes: [256, 256]
  tau: 0.005

# Entropy / temperature
temperature:
  init_alpha: 0.2
  target_entropy_scale: 1.0
  learning_rate: 1e-4

# Training schedule
training_frame_num: 32 # frames per collector batch
batch_size: 512
replay_capacity: 200000
warmup_steps: 160000 # collect before learning starts
updates_per_step: 0.25 # gradient steps per env frame
policy_update_freq: 2
vf_update_freq: 2
